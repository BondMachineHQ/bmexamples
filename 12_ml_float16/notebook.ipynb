{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *This notebook provides a full example on how to build a firmware for Machine Learning (Deep Learning) Inference on FPGA through the BondMachine framework. The are two main steps: in the first one, the user has to train a model and create a BondMachine Deep Learning firmware through pybondmachine, a library developed to abstract the BondMachine framework. In the second step, the user will use the firmware to perform inference on FPGA.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.11.0\n",
    "!pip install --upgrade pybondmachine\n",
    "!pip install typing-extensions --upgrade\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Env variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`BONDMACHINE_DIR`** - the directory where bondmachine tools are installed\n",
    "- **`XILINX_HLS`** - the directory where Xilinx HLS is installed\n",
    "- **`XILINX_VIVADO`** - the directory where Xilinx Vivado is installed\n",
    "- **`XILINX_VITIS`** - the directory where Xilinx Vitis is installed\n",
    "- **`PATH`** - add the bondmachine tools to the path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "BONDMACHINE_DIR=\"/home/\"+os.environ[\"USER\"]+\"/bin\"\n",
    "os.environ[\"BONDMACHINE_DIR\"]=BONDMACHINE_DIR\n",
    "os.environ[\"PATH\"]=os.environ[\"PATH\"]+\":\"+os.environ[\"BONDMACHINE_DIR\"]\n",
    "os.environ['XILINX_HLS'] = '/tools/Xilinx/Vitis_HLS/2023.2'\n",
    "os.environ['XILINX_VIVADO'] = '/tools/Xilinx/Vivado/2023.2'\n",
    "os.environ['XILINX_VITIS'] = '/tools/Xilinx/Vitis/2023.2'\n",
    "os.environ['PATH']=os.environ[\"PATH\"]+\":\"+os.environ['XILINX_HLS']+\"/bin:\"+os.environ['XILINX_VIVADO']+\"/bin:\"+os.environ['XILINX_VITIS']+\"/bin:\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`os`** - provides a portable way of using operating system dependent functionality\n",
    "- **`sklearn`** - provides simple and efficient tools for predictive data analysis\n",
    "- **`numpy`** - provides a fast numerical array structure and helper functions\n",
    "- **`csv`** - provides classes for reading and writing tabular data in CSV format\n",
    "- **`matplotlib`** - provides a MATLAB-like plotting framework\n",
    "- **`tensorflow`** - provides an open source machine learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "import tensorflow.compat.v1 as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the dataset from openML. OpenML is a platform for sharing machine learning data, tasks and experiments. It's a great place to find datasets to work with. You can find the dataset we are going to use here: https://www.openml.org/d/42468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = \"banknote-authentication\"\n",
    "data = fetch_openml(dataset)\n",
    "x_data, y_data = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Process data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process the data, split the data in train and tast sample and save them for later use. The test sample will be used to test the model on the hardware and to compare the results with the model running on FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder() # create a new instance of label encoder class, which is used to encode categorical labels as numerical values\n",
    "y = le.fit_transform(y_data) # this methods fits the encoder to the unique values in y_data and then transforms y_data into an array of encoded values\n",
    "unique = np.unique(y) # get the unique values in y\n",
    "y = to_categorical(y, len(unique)) # convert the encoded y array into a one-hot encoded matrix. len(unique) is the number of classes in the dataset\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(x_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# perform feature scaling on the training and testing data using the StandardScaler class from sklearn.preprocessing module.\n",
    "scaler = StandardScaler()\n",
    "X_train_val = scaler.fit_transform(X_train_val) # fit the scaler to the training data and then transform it\n",
    "X_test = scaler.transform(X_test) # transform the test data using the scaler that was fit to the training data\n",
    "classes = le.classes_\n",
    "classes_len = len(classes)\n",
    "\n",
    "if not os.path.exists('datasets'):\n",
    "    os.makedirs('datasets')\n",
    "\n",
    "np.save(\"datasets/\"+dataset+'_X_train_val.npy', X_train_val)\n",
    "np.save(\"datasets/\"+dataset+'_X_test.npy', X_test)\n",
    "np.save(\"datasets/\"+dataset+'_y_train_val.npy', y_train_val)\n",
    "np.save(\"datasets/\"+dataset+'_y_test.npy', y_test)\n",
    "np.save(\"datasets/\"+dataset+'_classes.npy', le.classes_)\n",
    "\n",
    "with open(\"datasets/\"+dataset+'_sample.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Neural Network model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the model starting from a JSON file that describes the network. The network architecture as well as the training process is not the main focus of this notebook, the complexity of the network is not important and it is very simple. The main focus is to show how to use the bondmachine tools to accelerate the inference of a neural network on FPGA. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat nn-specifics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = X_train_val.shape[0]\n",
    "model = Sequential()\n",
    "f = open('nn-specifics.json')\n",
    "network_spec = json.load(f)\n",
    "f.close()\n",
    "\n",
    "arch = network_spec[\"network\"][\"arch\"]\n",
    "for i in range(0, len(arch)):\n",
    "    layer_name = network_spec[\"network\"][\"arch\"][i][\"layer_name\"]\n",
    "    activation_function = network_spec[\"network\"][\"arch\"][i][\"activation_function\"]\n",
    "    neurons = network_spec[\"network\"][\"arch\"][i][\"neurons\"]\n",
    "    if i == 0:\n",
    "        model.add(Dense(neurons, input_shape=(X_train_val.shape[1],), kernel_regularizer=l1(0.0001)))\n",
    "    else:\n",
    "        model.add(Dense(neurons, activation=activation_function, name=layer_name, kernel_regularizer=l1(0.0001)))\n",
    "\n",
    "if  network_spec[\"network\"][\"training\"][\"optimizer\"] == \"Adam\":\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "elif network_spec[\"network\"][\"training\"][\"optimizer\"] == \"Adagrad\":\n",
    "    opt = Adagrad(learning_rate=0.0001)\n",
    "else:\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.add(Dense(classes_len, activation='softmax'))\n",
    "model.compile(optimizer=opt, loss=['categorical_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model and save the predictions for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this block of code we train the neural network and save the model.\n",
    "checkpoint_path = 'models/'+dataset+'/training/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True)\n",
    "batch_size = int(X_train_val.shape[1]*10) if network_spec[\"network\"][\"training\"][\"batch_size\"] == \"default\" else int(network_spec[\"network\"][\"training\"][\"batch_size\"])\n",
    "epochs = int(network_spec[\"network\"][\"training\"][\"epochs\"])\n",
    "validation_split = 0.25 if network_spec[\"network\"][\"training\"][\"validation_split\"] == \"default\" else float(network_spec[\"network\"][\"training\"][\"validation_split\"])\n",
    "shuffle = True if network_spec[\"network\"][\"training\"][\"shuffle\"] == \"true\" else False\n",
    "\n",
    "model.fit(X_train_val, y_train_val, batch_size=batch_size, epochs=epochs, validation_split=validation_split, shuffle=shuffle, callbacks=[cp_callback], verbose=0)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Dump the keras predictions of the test sample to be used as cross validation for the BondMachine NN\n",
    "y_keras = model.predict(X_test)\n",
    "np.save(\"datasets/\"+dataset+'_y_keras.npy', y_keras)\n",
    "\n",
    "results = [[*pred, np.argmax(pred)] for pred in y_keras]\n",
    "fields = ['probability_' + str(i) for i in range(len(classes))] + ['classification']\n",
    "\n",
    "with open(\"datasets/\"+dataset+'_sw.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(fields)\n",
    "    write.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls models/banknote-authentication/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **BONDMACHINE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the `pybondmachine` library to create, setup and build the firmware. Start from the basic import #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pybondmachine.prjmanager.prjhandler import BMProjectHandler\n",
    "from pybondmachine.converters.tensorflow2bm import mlp_tf2bm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert the trained neural network model into a standard file intepretable by `neuralbond`, the BondMachine tool that will convert the model into a set of heterogeneus connecting processors that represent the neural network #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_file = \"modelBM.json\"\n",
    "output_path = os.getcwd()+\"/output/\"\n",
    "\n",
    "mlp_tf2bm(model, output_file=output_file, output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the `BondMachine` object, this object will be used to create the project, setup configuration and build the firmeare #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prjHandler = BMProjectHandler(\"sample_project\", \"neuralnetwork\", \"projects_tests\")\n",
    "prjHandler.check_dependencies() # bmhelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prjHandler.create_project(target_board='alveou50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the configuration of the project, in details:\n",
    "- **`data_type`** - the data type of the bondmachine architecture\n",
    "- **`register_size`** - the size of the register in the bondmachine architecture\n",
    "- **`source_neuralbond`** - the path to the neuralbond source file\n",
    "- **`flavor`** - the interconnection protocol used to interact with the firmware\n",
    "- **`board`** - the target device where the firmware will run\n",
    "\n",
    "##### and then setup the configuration of the project #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data_type\": \"float16\",\n",
    "    \"register_size\": \"16\",\n",
    "    \"source_neuralbond\": output_path+output_file,\n",
    "    \"flavor\": \"axist\",\n",
    "    \"board\": \"alveou50\"\n",
    "}\n",
    "\n",
    "prjHandler.setup_project(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, build the firmware. The `oncloud` parameter is used to specify if the firmware will be built on the cloud or locally. Build the firmware using this library is not actually supported, but we have developed a prototype of Inference As a Service System that includes the build firmware process on a remote host #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will take more than an 1 hour, some firmwares have already been prepared\n",
    "#prjHandler.build_firmware(oncloud=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['XILINX_XRT'] = '/opt/xilinx/xrt'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/opt/xilinx/xrt/lib'\n",
    "\n",
    "notebook_directory = os.path.abspath(os.path.dirname((os.environ[\"JPY_SESSION_NAME\"])))\n",
    "os.chdir(notebook_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the bitstream file from bondmachine.fisica.unipg.it\n",
    "!wget -nc http://bondmachine.fisica.unipg.it/firmwares/bmfloat16.xclbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pybondmachine.overlay.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model_specs = {\n",
    "    \"data_type\": \"float16\",\n",
    "    \"register_size\": 16,\n",
    "    \"batch_size\": 16,\n",
    "    \"flavor\": \"axist\",\n",
    "    \"n_input\": 4,\n",
    "    \"n_output\": 2,\n",
    "    \"benchcore\": True,\n",
    "    \"board\": \"alveo\"\n",
    "}\n",
    "firmware_name = \"bmfloat16.xclbin\"\n",
    "firmware_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"datasets/banknote-authentication_X_test.npy\")\n",
    "y_test = np.load(\"datasets/banknote-authentication_y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = Predictor(firmware_name, firmware_path, model_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.load_overlay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.prepare_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status, result = predictor.predict(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
