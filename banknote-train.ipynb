{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from openML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"banknote-authentication\"\n",
    "data = fetch_openml(dataset)\n",
    "x_data, y_data = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_data)\n",
    "unique = np.unique(y)\n",
    "y = to_categorical(y, len(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(x_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_val = scaler.fit_transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "classes = le.classes_\n",
    "classes_len = len(classes)\n",
    "\n",
    "np.save(\"datasets/\"+dataset+'_X_train_val.npy', X_train_val)\n",
    "np.save(\"datasets/\"+dataset+'_X_test.npy', X_test)\n",
    "np.save(\"datasets/\"+dataset+'_y_train_val.npy', y_train_val)\n",
    "np.save(\"datasets/\"+dataset+'_y_test.npy', y_test)\n",
    "np.save(\"datasets/\"+dataset+'_classes.npy', le.classes_)\n",
    "\n",
    "with open(\"datasets/\"+dataset+'_sample.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerows(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = X_train_val.shape[0]\n",
    "model = Sequential()\n",
    "f = open('specifics.json')\n",
    "network_spec = json.load(f)\n",
    "\n",
    "arch = network_spec[\"network\"][\"arch\"]\n",
    "for i in range(0, len(arch)):\n",
    "    layer_name = network_spec[\"network\"][\"arch\"][i][\"layer_name\"]\n",
    "    activation_function = network_spec[\"network\"][\"arch\"][i][\"activation_function\"]\n",
    "    neurons = network_spec[\"network\"][\"arch\"][i][\"neurons\"]\n",
    "    if i == 0:\n",
    "        model.add(Dense(neurons, input_shape=(X_train_val.shape[1],), kernel_regularizer=l1(0.0001)))\n",
    "    else:\n",
    "        model.add(Dense(neurons, activation=activation_function, name=layer_name, kernel_regularizer=l1(0.0001)))\n",
    "\n",
    "if  network_spec[\"network\"][\"training\"][\"optimizer\"] == \"Adam\":\n",
    "    opt = Adam(lr=0.0001)\n",
    "elif network_spec[\"network\"][\"training\"][\"optimizer\"] == \"Adagrad\":\n",
    "    opt = Adagrad(lr=0.0001)\n",
    "else:\n",
    "    opt = Adam(lr=0.0001)\n",
    "\n",
    "model.add(Dense(classes_len, activation='softmax'))\n",
    "model.compile(optimizer=opt, loss=['categorical_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'models/'+dataset+'/training/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1)\n",
    "batch_size = int(X_train_val.shape[1]*10) if network_spec[\"network\"][\"training\"][\"batch_size\"] == \"default\" else int(network_spec[\"network\"][\"training\"][\"batch_size\"])\n",
    "epochs = int(network_spec[\"network\"][\"training\"][\"epochs\"])\n",
    "validation_split = 0.25 if network_spec[\"network\"][\"training\"][\"validation_split\"] == \"default\" else float(network_spec[\"network\"][\"training\"][\"validation_split\"])\n",
    "shuffle = True if network_spec[\"network\"][\"training\"][\"shuffle\"] == \"true\" else False\n",
    "\n",
    "model.fit(\n",
    "    X_train_val, \n",
    "    y_train_val, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    validation_split=validation_split, \n",
    "    shuffle=shuffle,\n",
    "    callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUMP MODEL FOR NEURALBOND\n",
    "\n",
    "layers = model.layers\n",
    "weights = model.weights\n",
    "\n",
    "to_dump = {}\n",
    "\n",
    "weights = []\n",
    "nodes = []\n",
    "\n",
    "# save weigths\n",
    "for i in range(0 , len(layers)):\n",
    "\n",
    "    layer_weights = layers[i].get_weights()\n",
    "\n",
    "    for m in range(0, len(layer_weights)):\n",
    "        for w in range(0, len(layer_weights[m])):\n",
    "            try:\n",
    "                for v in range(0, len(layer_weights[m][w])):\n",
    "                    weight_info = {\n",
    "                            \"Layer\": i+1,\n",
    "                            \"PosCurrLayer\": v,\n",
    "                            \"PosPrevLayer\": w,\n",
    "                            \"Value\": float(layer_weights[m][w][v])\n",
    "                        }\n",
    "                    weights.append(weight_info)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    #print(weights)\n",
    "    #flat_layer_weigth = [item for sublist in layer_weights for item in sublist]\n",
    "    if i == 0:\n",
    "        for units in range(0, layers[i].units):\n",
    "            weights_l0 = layers[i].get_weights()[0]\n",
    "            for w in range(0, len(weights_l0)):\n",
    "                node_info = {\n",
    "                    \"Layer\": 0,\n",
    "                    \"Pos\": w,\n",
    "                    \"Type\": \"input\",\n",
    "                    \"Bias\": 0\n",
    "                }\n",
    "                nodes.append(node_info)\n",
    "            break\n",
    "    \n",
    "\n",
    "    for units in range(0, layers[i].units):\n",
    "        if i == len(layers) - 1:\n",
    "            bias = layers[i].get_weights()[1]\n",
    "            node_info = {\n",
    "                \"Layer\": i+1,\n",
    "                \"Pos\": units,\n",
    "                \"Type\": \"summation\",\n",
    "                \"Bias\": bias.tolist()[units]\n",
    "            }\n",
    "            nodes.append(node_info)\n",
    "        else:\n",
    "            bias = layers[i].get_weights()[1]\n",
    "            node_info = {\n",
    "                \"Layer\": i+1,\n",
    "                \"Pos\": units,\n",
    "                \"Type\": layers[i].activation.__name__,\n",
    "                \"Bias\": bias.tolist()[units]\n",
    "            }\n",
    "            nodes.append(node_info)\n",
    "\n",
    "\n",
    "    if i == len(layers) - 1:\n",
    "        for l in range(0, len(layer_weights[0][0])):\n",
    "            node_info = {\n",
    "                \"Layer\": i+2,\n",
    "                \"Pos\": l,\n",
    "                \"Type\": \"softmax\",\n",
    "                \"Bias\": 0\n",
    "            }\n",
    "            nodes.append(node_info)\n",
    "\n",
    "\n",
    "        layer_weights = layers[i-2].get_weights()\n",
    "\n",
    "        for k in range(0, layers[i].units):\n",
    "            for l in range(0, layers[i].units):\n",
    "                weight_info = {\n",
    "                                \"Layer\": i+2,\n",
    "                                \"PosCurrLayer\": k,\n",
    "                                \"PosPrevLayer\": l,\n",
    "                                \"Value\": 1\n",
    "                            }\n",
    "                weights.append(weight_info)\n",
    "\n",
    "\n",
    "    if i == len(layers) - 1:\n",
    "        for l in range(0, len(layer_weights[0][0])):\n",
    "            node_info = {\n",
    "                \"Layer\": i+3,\n",
    "                \"Pos\": l,\n",
    "                \"Type\": \"output\",\n",
    "                \"Bias\": 0\n",
    "            }\n",
    "            nodes.append(node_info)\n",
    "\n",
    "            weight_info = {\n",
    "                            \"Layer\": i+3,\n",
    "                            \"PosCurrLayer\": l,\n",
    "                            \"PosPrevLayer\": l,\n",
    "                            \"Value\": 1\n",
    "                        }\n",
    "            weights.append(weight_info)\n",
    "\n",
    "to_dump[\"Nodes\"] = nodes\n",
    "to_dump[\"Weights\"] = weights\n",
    "\n",
    "with open('models/'+dataset+'/modelBM.json', 'w') as fp:\n",
    "    print(\" *** dump model\")\n",
    "    json.dump(to_dump, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUMP KERAS PREDICTION\n",
    "y_keras = model.predict(X_test)\n",
    "np.save(\"datasets/\"+dataset+'_y_keras.npy', y_keras)\n",
    "results = []\n",
    "for pred in y_keras:\n",
    "    prediction = np.argmax(pred, axis=0)\n",
    "    to_save = [pred[0], pred[1] ,prediction]\n",
    "    results.append(to_save)\n",
    "    \n",
    "fields = ['probability_0', 'probability_1', 'classification'] \n",
    "\n",
    "with open(\"datasets/\"+dataset+'_swprediction.csv', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(fields)\n",
    "    write.writerows(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
